<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Junjie Ye</title>
    <link>/</link>
      <atom:link href="/index.xml" rel="self" type="application/rss+xml" />
    <description>Junjie Ye</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Mon, 01 Mar 2021 00:00:00 +0000</lastBuildDate>
    <image>
      <url>/images/icon_hu9902be2b97aa5e307a6e65af56d961d4_168654_512x512_fill_lanczos_center_2.png</url>
      <title>Junjie Ye</title>
      <link>/</link>
    </image>
    
    <item>
      <title>Predictive Visual Tracking: A New Benchmark and Baseline Approach</title>
      <link>/publication/2021_arxiv_pvt/</link>
      <pubDate>Mon, 01 Mar 2021 00:00:00 +0000</pubDate>
      <guid>/publication/2021_arxiv_pvt/</guid>
      <description>&lt;center&gt;
![ADTrack_workflow](featured.png)
&lt;small&gt;&lt;/small&gt;
&lt;/center&gt;</description>
    </item>
    
    <item>
      <title>ADTrack: Target-Aware Dual Filter Learning for Real-Time Anti-Dark UAV Tracking</title>
      <link>/publication/2021_icra_adtrack/</link>
      <pubDate>Sun, 28 Feb 2021 08:00:00 +0000</pubDate>
      <guid>/publication/2021_icra_adtrack/</guid>
      <description>&lt;center&gt;
![MKCT_workflow](featured.png)
&lt;small&gt;Overall framework of the proposed ADTrack. ADTrack includes 3 stages: pretreatment, training, and detection, which are marked out by boxes in different colors. Dual filters, i.e., context filter and target-focused filter, training and detection follow routes in different colors. It can be seen that the final response shaded noises in context response, which indicates the validity of proposed dual filter.&lt;/small&gt;
&lt;/center&gt;
</description>
    </item>
    
    <item>
      <title>Mutation Sensitive Correlation Filter for Real-Time UAV Tracking with Adaptive Hybrid Label</title>
      <link>/publication/2021_icra_mscf_tracker/</link>
      <pubDate>Sun, 28 Feb 2021 08:00:00 +0000</pubDate>
      <guid>/publication/2021_icra_mscf_tracker/</guid>
      <description>&lt;center&gt;
![MKCT_workflow](featured.png)
&lt;small&gt;Tracking procedure of the proposed MSCF tracker. Dashed boxes denote the variables to be solved in the main regression. As MTF in the red
box is generated from search region in frame k, it is applied to adjust the altitude value of the cruciform pedestal.&lt;/small&gt;
&lt;/center&gt;
</description>
    </item>
    
    <item>
      <title>Siamese Anchor Proposal Network for High-Speed Aerial Tracking</title>
      <link>/publication/2021_icra_siamapn/</link>
      <pubDate>Sun, 28 Feb 2021 08:00:00 +0000</pubDate>
      <guid>/publication/2021_icra_siamapn/</guid>
      <description>&lt;center&gt;
&lt;p&gt;&lt;img src=&#34;featured.png&#34; alt=&#34;MKCT_workflow&#34;&gt;
&lt;small&gt;The overview of SiamAPN tracker. It composes of four subnetworks, i.e., feature extraction network, feature fusion network, anchor proposal network (APN), and muti-classification®ression network.&lt;/small&gt;&lt;/p&gt;
&lt;/center&gt;
</description>
    </item>
    
    <item>
      <title>All-Day Object Tracking for Unmanned Aerial Vehicle</title>
      <link>/publication/2021_arxiv_adtrack/</link>
      <pubDate>Sun, 24 Jan 2021 00:00:00 +0000</pubDate>
      <guid>/publication/2021_arxiv_adtrack/</guid>
      <description>&lt;center&gt;
![ADTrack_workflow](featured.jpg)
&lt;small&gt;Pipeline of ADTrack.&lt;/small&gt;
&lt;/center&gt;</description>
    </item>
    
    <item>
      <title>UAVDark135</title>
      <link>/project/uavdark135/</link>
      <pubDate>Sat, 31 Oct 2020 00:00:00 +0000</pubDate>
      <guid>/project/uavdark135/</guid>
      <description>&lt;p&gt;UAVDark135 is the very first UAV dark tracking benchmark dedicated to providing a comprehensive evaluation of tracking performance at night.&lt;/p&gt;
&lt;p&gt;UAVDark135 consists of 135 sequences, most of which were shot by a standard UAV at night, including more than 125k manually annotated frames. The benchmark covers a wide range of scenes, e.g., road, ocean, street, highway, and lakeside, including a large number of objects, such as person, car, building, athlete, truck, and bike.&lt;/p&gt;
&lt;p&gt;The benchmark is available &lt;a href=&#34;https://pan.baidu.com/s/1JcV_wTUSt9F8iBXiLCZQdQ&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt; (password: axci).&lt;/p&gt;
&lt;h3 id=&#34;uavdark135-tracking-benchmark&#34;&gt;UAVDark135 Tracking Benchmark&lt;/h3&gt;
&lt;h4 id=&#34;a-platform-and-statistics&#34;&gt;A. Platform and Statistics&lt;/h4&gt;
&lt;p&gt;Standing as the first UAV dark tracking benchmark, the UAVDark135 contains totally 135 sequences captured by a standard UAV2 at night. The benchmark includes various tracking scenes, e.g., crossings, t-junctions, road, highway, and consists of different kinds of tracked objects like people, boat, bus, car, truck, athletes, house, etc. To extent the covered scenes, the benchmark also contains some sequences from YouTube, which were shot on the sea. The total frames, mean frames, maximum frames, and minimum frames of the benchmark are 125466, 929, 4571, and 216 respectively, making it suitable for large-scale evaluation. The videos are captured at a frame-rate of 30 frames/s (FPS), with the resolution of 1920×1080.&lt;/p&gt;
&lt;h4 id=&#34;b-annotation&#34;&gt;B. Annotation&lt;/h4&gt;
&lt;p&gt;The frames in UAVDark135 are all manually annotated, where a sequence is completely processed by the same annotator to ensure consistency. Since in some dark scenes the object is nearly invisible, annotation process is much more strenuous. After the first round, 5 professional annotators carefully checked the results and made revision for several rounds to reduce errors as much as possible in nearly 2 months.&lt;/p&gt;
&lt;p&gt;Since the boundary contour of the object is not obvious in the dark, the result boxes of the first annotation fluctuates in continuous image frames. However, the actual motion process of the object should be smooth. In these considerations, we record the original annotation every 5 frames for the sequence with extremely severe vibration, and the results of the remaining frames are obtained by linear interpolation, which is closer to the position and scale variation of the real object.&lt;/p&gt;
&lt;h4 id=&#34;c-attributes&#34;&gt;C. Attributes&lt;/h4&gt;
&lt;p&gt;&lt;img src=&#34;image-20210203003923661.png&#34; alt=&#34;image-20210203003923661&#34;&gt;&lt;/p&gt;
&lt;p&gt;For more details, please refer to our &lt;a href=&#34;https://arxiv.org/abs/2101.08446&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;paper&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Disruptor-Aware Interval-Based Response Inconsistency for Correlation Filters in Real-Time Aerial Tracking</title>
      <link>/publication/2020_tgrs_ibri-tracker/</link>
      <pubDate>Wed, 07 Oct 2020 00:00:00 +0000</pubDate>
      <guid>/publication/2020_tgrs_ibri-tracker/</guid>
      <description>&lt;center&gt;
![IBRI_workflow](featured.jpg)
&lt;small&gt;Tracking procedure of the proposed IBRI tracker in the k-th frame. Historical interval responses are incorporated into the filter training phase after denoising by a novel disruptor-aware scheme based on response bucketing.&lt;/small&gt;
&lt;/center&gt;</description>
    </item>
    
  </channel>
</rss>
